{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport os\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-20T01:34:06.124029Z","iopub.execute_input":"2023-10-20T01:34:06.124240Z","iopub.status.idle":"2023-10-20T01:34:06.430821Z","shell.execute_reply.started":"2023-10-20T01:34:06.124220Z","shell.execute_reply":"2023-10-20T01:34:06.429937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Read News and DJIA Data\n# df_news = pd.read_csv('/kaggle/input/financialnews/Combined_News_DJIA.csv')\n# df_stock = pd.read_csv('/kaggle/input/financialnews/upload_DJIA_table.csv')\ndf_news = pd.read_csv('/kaggle/input/masterthesis-00-datagathering/SP500_all.csv')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-20T01:44:03.317145Z","iopub.execute_input":"2023-10-20T01:44:03.317466Z","iopub.status.idle":"2023-10-20T01:44:04.145144Z","shell.execute_reply.started":"2023-10-20T01:44:03.317444Z","shell.execute_reply":"2023-10-20T01:44:04.144452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check data\ndisplay(df_news.info())\n# print('______________________________________________')\n# display(df_stock.info())","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-20T01:44:07.004903Z","iopub.execute_input":"2023-10-20T01:44:07.005214Z","iopub.status.idle":"2023-10-20T01:44:07.020361Z","shell.execute_reply.started":"2023-10-20T01:44:07.005190Z","shell.execute_reply":"2023-10-20T01:44:07.019538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_news.drop('Unnamed: 0', axis=1, inplace=True)\n# df_news.drop('Unnamed: 0', axis=1, inplace=True)\n# df_news.drop('text', axis=1, inplace=True)\n\ndf_news = df_news[['date','headline']]","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-20T01:45:31.509301Z","iopub.execute_input":"2023-10-20T01:45:31.509681Z","iopub.status.idle":"2023-10-20T01:45:31.515436Z","shell.execute_reply.started":"2023-10-20T01:45:31.509629Z","shell.execute_reply":"2023-10-20T01:45:31.514553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adjust display settings\n\npd.set_option('display.max_colwidth', None)\npd.set_option('display.expand_frame_repr', False)\n\n# Get 5 random samples with reproducibility\ndisplay(df_news.sample(n=5, random_state=42))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-20T01:45:34.900988Z","iopub.execute_input":"2023-10-20T01:45:34.901333Z","iopub.status.idle":"2023-10-20T01:45:34.910569Z","shell.execute_reply.started":"2023-10-20T01:45:34.901306Z","shell.execute_reply":"2023-10-20T01:45:34.909760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_news.fillna('', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:45:43.429429Z","iopub.execute_input":"2023-10-20T01:45:43.429810Z","iopub.status.idle":"2023-10-20T01:45:43.437963Z","shell.execute_reply.started":"2023-10-20T01:45:43.429785Z","shell.execute_reply":"2023-10-20T01:45:43.437006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nfrom nltk.stem import WordNetLemmatizer\nimport re\nimport html\n\n# Initialize the lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# Define the stop words\nstop_words = set(stopwords.words('english'))\n\n# Define a function to preprocess the headlines\ndef preprocess_text(text, mode=\"simple\"):\n    # Convert the text to lower case\n    #text = text.lower()\n    \n    # Remove html entities\n    text = html.unescape(text)\n    \n    # Remove byte-string prefix and content\n    text = re.sub(r\"b'\", \"\", text)\n    text = re.sub(r'b\"', \"\", text)\n    text = re.sub(r\"\\r\", \"\", text)\n    text = re.sub(r\"\\n\", \"\", text)\n    text = re.sub(r\"\\\\\", \"\", text)\n    \n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Remove mentions\n    text = re.sub(r'@\\w+', '', text)\n    \n    # Remove numeric values\n    text = re.sub(r'\\d+', '', text)\n    \n    if mode == \"advanced\":   \n    \n        # Remove punctuation\n        text = re.sub(r'[^\\w\\s]', '', text)\n\n        # Tokenize the text\n        text = text.split()\n\n        # Remove stop words and lemmatize the words\n        text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]\n        \n        # Join the words back into a single string\n        text = ' '.join(text)\n        \n    else:\n        \n        # Join the words back into a single string\n        text = ''.join(text)\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:45:51.017798Z","iopub.execute_input":"2023-10-20T01:45:51.018657Z","iopub.status.idle":"2023-10-20T01:45:51.027575Z","shell.execute_reply.started":"2023-10-20T01:45:51.018625Z","shell.execute_reply":"2023-10-20T01:45:51.026795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # New columns for preprocessed headlines\n# new_cols_simple = [f'Top{i}_Preprocessed' for i in range(1, 26)]\n# # new_cols_advanced = [f'Top{i}_Preprocessed_Advanced' for i in range(1, 26)]\n\n# # Apply the function to each headline\n# for i in range(1, 26):\n#     col_name = f'Top{i}'\n#     new_col_simple = f'Top{i}_Preprocessed'\n# #     new_col_advanced = f'Top{i}_Preprocessed_Advanced'\n    \n#     df_news[new_col_simple] = df_news[col_name].apply(preprocess_text, mode=\"simple\")\n# #     df_news[new_col_advanced] = df_news[col_name].apply(preprocess_text, mode=\"advanced\")\n\ndf_news['headline_preprocessed'] = df_news['headline'].apply(preprocess_text, mode=\"simple\")","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:45:55.484965Z","iopub.execute_input":"2023-10-20T01:45:55.485275Z","iopub.status.idle":"2023-10-20T01:45:55.639521Z","shell.execute_reply.started":"2023-10-20T01:45:55.485251Z","shell.execute_reply":"2023-10-20T01:45:55.638523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_news.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:46:05.089352Z","iopub.execute_input":"2023-10-20T01:46:05.089947Z","iopub.status.idle":"2023-10-20T01:46:05.101255Z","shell.execute_reply.started":"2023-10-20T01:46:05.089920Z","shell.execute_reply":"2023-10-20T01:46:05.100453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import BertTokenizer, BertForSequenceClassification\n# from transformers import pipeline\n\n# finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n# tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n\n# nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer, top_k = 1)\n\n# sentences = df_news['headline_preprocessed'][:10].tolist()\n# results = nlp(sentences)\n# print(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\n\n# sentences = df_news['headline'][:10].tolist()\n# results = nlp(sentences)\n# print(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:46:09.272754Z","iopub.execute_input":"2023-10-20T01:46:09.273077Z","iopub.status.idle":"2023-10-20T01:46:09.276979Z","shell.execute_reply.started":"2023-10-20T01:46:09.273052Z","shell.execute_reply":"2023-10-20T01:46:09.276009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.nn.functional import softmax\n\n# Load pre-trained model and tokenizer from Hugging Face\nmodel_name = 'yiyanghkust/finbert-tone'  \ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name)\n\n# Use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ndef predict_sentiment_finbert(data, batch_size=32):\n    \"\"\"\n    Predicts the sentiment of text data in a DataFrame using FinBERT, with batching.\n    \n    Args:\n        data (pd.Series): Series containing the text data.\n        batch_size (int): The number of samples to process in each batch.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with sentiment scores and labels.\n    \"\"\"\n    scores = []\n    labels = []\n    prob_score = []\n\n    # Tokenize text and convert to input format for BERT\n    tokenized_output = tokenizer(data.tolist(), \n                                 padding=True, \n                                 truncation=True, \n                                 return_tensors=\"pt\", \n                                 max_length=512)\n    input_ids = tokenized_output['input_ids']\n    attention_mask = tokenized_output['attention_mask']\n    \n    # Move to device\n    input_ids = input_ids.to(device)\n    attention_mask = attention_mask.to(device)\n\n    # Process text in batches\n    for i in range(0, len(input_ids), batch_size):\n        inputs = input_ids[i : i + batch_size]\n\n        # Model inference\n        with torch.no_grad():\n            outputs = model(inputs, attention_mask=attention_mask[i : i + batch_size])\n        \n        # Get probabilities\n        probs = softmax(outputs.logits, dim=1).cpu().numpy()\n        \n        # Convert to a score between -1 and 1\n        sentiment_scores = probs[:, 1] - probs[:, 2]  # LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\n        \n        # Get the sentiment label (assuming LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative)\n        sentiment_labels = ['Neutral', 'Positive', 'Negative']\n        sentiment_label_indices = probs.argmax(axis=1)\n        batch_labels = [sentiment_labels[idx] for idx in sentiment_label_indices]\n        \n        scores.extend(sentiment_scores)\n        labels.extend(batch_labels)\n        prob_score.extend(probs)\n\n    results = pd.DataFrame({\n        'headline': data,\n        'finbert_score': scores, \n        'label': labels,\n        'probs': prob_score\n    }, index=data.index)\n\n    return results\n\ndf_results = predict_sentiment_finbert(df_news['headline'])","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:46:09.678141Z","iopub.execute_input":"2023-10-20T01:46:09.678464Z","iopub.status.idle":"2023-10-20T01:46:28.993411Z","shell.execute_reply.started":"2023-10-20T01:46:09.678441Z","shell.execute_reply":"2023-10-20T01:46:28.992465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_news = pd.concat([df_news, df_results['finbert_score']], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:46:44.147514Z","iopub.execute_input":"2023-10-20T01:46:44.147847Z","iopub.status.idle":"2023-10-20T01:46:44.153653Z","shell.execute_reply.started":"2023-10-20T01:46:44.147823Z","shell.execute_reply":"2023-10-20T01:46:44.152649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_news.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:47:20.696898Z","iopub.execute_input":"2023-10-20T01:47:20.697221Z","iopub.status.idle":"2023-10-20T01:47:20.707052Z","shell.execute_reply.started":"2023-10-20T01:47:20.697197Z","shell.execute_reply":"2023-10-20T01:47:20.706144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import pandas as pd\n# from transformers import BertTokenizer, BertForSequenceClassification\n# from torch.nn.functional import softmax\n\n# # Load pre-trained model and tokenizer from Hugging Face\n# model_name = 'yiyanghkust/finbert-tone'  # Example FinBERT model, adjust as needed\n# tokenizer = BertTokenizer.from_pretrained(model_name)\n# model = BertForSequenceClassification.from_pretrained(model_name)\n\n# # Use GPU if available\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n\n# def predict_sentiment_finbert(data, batch_size=32):\n#     \"\"\"\n#     Predicts the sentiment of text data in a DataFrame using FinBERT, with batching.\n    \n#     Args:\n#         data (pd.Series): Series containing the text data.\n#         batch_size (int): The number of samples to process in each batch.\n        \n#     Returns:\n#         pd.Series: A series of sentiment scores between -1 and 1.\n#     \"\"\"\n#     scores = []\n\n#     # Tokenize text and convert to input format for BERT\n#     input_ids = tokenizer(data.tolist(), \n#                           padding=True, \n#                           truncation=True, \n#                           return_tensors=\"pt\", \n#                           max_length=512)['input_ids']\n    \n#     # Move to device\n#     input_ids = input_ids.to(device)\n\n#     # Process text in batches\n#     for i in range(0, len(input_ids), batch_size):\n#         inputs = input_ids[i : i + batch_size]\n\n#         # Model inference\n#         with torch.no_grad():\n#             outputs = model(inputs)\n        \n#         # Get probabilities\n#         probs = softmax(outputs.logits, dim=1).cpu().numpy()\n\n#         # Convert to a score between -1 and 1\n#         sentiment_scores = probs[:, 2] - probs[:, 0]  # Assuming 0: negative, 1: neutral, 2: positive\n        \n#         scores.extend(sentiment_scores)\n\n#     return pd.Series(scores, index=data.index)\n\n\n# # Call the function and add the resulting scores to df_news\n# df_news['finbert_score'] = predict_sentiment_finbert(df_news['headline'])","metadata":{"execution":{"iopub.status.busy":"2023-10-20T00:50:28.625388Z","iopub.execute_input":"2023-10-20T00:50:28.626115Z","iopub.status.idle":"2023-10-20T00:50:48.157398Z","shell.execute_reply.started":"2023-10-20T00:50:28.626085Z","shell.execute_reply":"2023-10-20T00:50:48.156441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install vaderSentiment","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:48:05.745045Z","iopub.execute_input":"2023-10-20T01:48:05.745647Z","iopub.status.idle":"2023-10-20T01:48:13.855315Z","shell.execute_reply.started":"2023-10-20T01:48:05.745611Z","shell.execute_reply":"2023-10-20T01:48:13.854175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\ndef predict_sentiment_vader(dataframe, text_column):\n    \"\"\"\n    Predicts the sentiment of text data in a DataFrame using VADER.\n    \n    Args:\n        dataframe (pd.DataFrame): DataFrame containing the text data.\n        text_column (str): The name of the column containing the text data.\n        \n    Returns:\n        pd.Series: A series of sentiment scores between -1 and 1.\n    \"\"\"\n    analyser = SentimentIntensityAnalyzer()\n    \n    # Compute sentiment scores\n    scores = dataframe[text_column].apply(lambda x: analyser.polarity_scores(x)['compound'])\n    \n    return scores\n\n\n# Call the function and add the resulting scores to df_news\ndf_news['vader_score'] = predict_sentiment_vader(df_news, 'headline_preprocessed')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:48:13.857781Z","iopub.execute_input":"2023-10-20T01:48:13.858467Z","iopub.status.idle":"2023-10-20T01:48:14.536638Z","shell.execute_reply.started":"2023-10-20T01:48:13.858438Z","shell.execute_reply":"2023-10-20T01:48:14.535650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_news","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:48:14.537778Z","iopub.execute_input":"2023-10-20T01:48:14.538032Z","iopub.status.idle":"2023-10-20T01:48:14.551464Z","shell.execute_reply.started":"2023-10-20T01:48:14.538010Z","shell.execute_reply":"2023-10-20T01:48:14.550538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_news.to_csv(\"sentiment.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:48:59.019513Z","iopub.execute_input":"2023-10-20T01:48:59.020390Z","iopub.status.idle":"2023-10-20T01:48:59.106207Z","shell.execute_reply.started":"2023-10-20T01:48:59.020359Z","shell.execute_reply":"2023-10-20T01:48:59.105292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_stock = pd.read_csv('/kaggle/input/masterthesis-00-datagathering/SP500_stock.csv')\ndf_stock.drop('Unnamed: 0', axis=1, inplace=True)\n\n# Calculate the average sentiment per day\ndf_news = df_news.groupby('date')[['finbert_score', 'vader_score']].mean()\ndf = df_news.merge(df_stock, on= \"date\")\n\ndf.to_csv(\"stock_sentiment.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:49:07.457137Z","iopub.execute_input":"2023-10-20T01:49:07.457917Z","iopub.status.idle":"2023-10-20T01:49:07.504560Z","shell.execute_reply.started":"2023-10-20T01:49:07.457890Z","shell.execute_reply":"2023-10-20T01:49:07.503820Z"},"trusted":true},"execution_count":null,"outputs":[]}]}