{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d68428a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-20T02:21:33.079143Z",
     "iopub.status.busy": "2023-10-20T02:21:33.078662Z",
     "iopub.status.idle": "2023-10-20T02:21:33.476708Z",
     "shell.execute_reply": "2023-10-20T02:21:33.474940Z"
    },
    "papermill": {
     "duration": 0.409927,
     "end_time": "2023-10-20T02:21:33.480355",
     "exception": false,
     "start_time": "2023-10-20T02:21:33.070428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/financialnews/upload_DJIA_table.csv\n",
      "/kaggle/input/financialnews/Combined_News_DJIA.csv\n",
      "/kaggle/input/masterthesis-01-seniment/__results__.html\n",
      "/kaggle/input/masterthesis-01-seniment/stock_sentiment.csv\n",
      "/kaggle/input/masterthesis-01-seniment/sentiment.csv\n",
      "/kaggle/input/masterthesis-01-seniment/__notebook__.ipynb\n",
      "/kaggle/input/masterthesis-01-seniment/__output__.json\n",
      "/kaggle/input/masterthesis-01-seniment/custom.css\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d8d49f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T02:21:33.494312Z",
     "iopub.status.busy": "2023-10-20T02:21:33.493503Z",
     "iopub.status.idle": "2023-10-20T02:21:33.523941Z",
     "shell.execute_reply": "2023-10-20T02:21:33.522548Z"
    },
    "papermill": {
     "duration": 0.038953,
     "end_time": "2023-10-20T02:21:33.526540",
     "exception": false,
     "start_time": "2023-10-20T02:21:33.487587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Read News and DJIA Data\n",
    "# df_news = pd.read_csv('/kaggle/input/financialnews/Combined_News_DJIA.csv')\n",
    "# stock_data = pd.read_csv('/kaggle/input/financialnews/upload_DJIA_table.csv')\n",
    "df = pd.read_csv('/kaggle/input/masterthesis-01-seniment/stock_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0476578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T02:21:33.537618Z",
     "iopub.status.busy": "2023-10-20T02:21:33.537178Z",
     "iopub.status.idle": "2023-10-20T02:21:33.559910Z",
     "shell.execute_reply": "2023-10-20T02:21:33.558669Z"
    },
    "papermill": {
     "duration": 0.031554,
     "end_time": "2023-10-20T02:21:33.562720",
     "exception": false,
     "start_time": "2023-10-20T02:21:33.531166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5461d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T02:21:33.573427Z",
     "iopub.status.busy": "2023-10-20T02:21:33.573030Z",
     "iopub.status.idle": "2023-10-20T02:21:33.597091Z",
     "shell.execute_reply": "2023-10-20T02:21:33.595936Z"
    },
    "papermill": {
     "duration": 0.03287,
     "end_time": "2023-10-20T02:21:33.600125",
     "exception": false,
     "start_time": "2023-10-20T02:21:33.567255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>finbert_score</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>-0.999072</td>\n",
       "      <td>-0.296000</td>\n",
       "      <td>2038.199951</td>\n",
       "      <td>2012.660034</td>\n",
       "      <td>2038.199951</td>\n",
       "      <td>1989.680054</td>\n",
       "      <td>4304880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>-0.849274</td>\n",
       "      <td>-0.073340</td>\n",
       "      <td>2013.780029</td>\n",
       "      <td>2016.709961</td>\n",
       "      <td>2021.939941</td>\n",
       "      <td>2004.170044</td>\n",
       "      <td>3706620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>-0.999978</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>2011.709961</td>\n",
       "      <td>1990.260010</td>\n",
       "      <td>2011.709961</td>\n",
       "      <td>1979.050049</td>\n",
       "      <td>4336660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>-0.884580</td>\n",
       "      <td>-0.309633</td>\n",
       "      <td>1985.319946</td>\n",
       "      <td>1943.089966</td>\n",
       "      <td>1985.319946</td>\n",
       "      <td>1938.829956</td>\n",
       "      <td>5076590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>-0.999922</td>\n",
       "      <td>-0.734550</td>\n",
       "      <td>1945.969971</td>\n",
       "      <td>1922.030029</td>\n",
       "      <td>1960.400024</td>\n",
       "      <td>1918.459961</td>\n",
       "      <td>4664940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>2022-12-22</td>\n",
       "      <td>-0.470038</td>\n",
       "      <td>0.037473</td>\n",
       "      <td>3853.260010</td>\n",
       "      <td>3822.389893</td>\n",
       "      <td>3853.260010</td>\n",
       "      <td>3764.489990</td>\n",
       "      <td>3956950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>0.495882</td>\n",
       "      <td>-0.023078</td>\n",
       "      <td>3815.110107</td>\n",
       "      <td>3844.820068</td>\n",
       "      <td>3845.800049</td>\n",
       "      <td>3797.010010</td>\n",
       "      <td>2819280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>-0.173062</td>\n",
       "      <td>0.155650</td>\n",
       "      <td>3843.340088</td>\n",
       "      <td>3829.250000</td>\n",
       "      <td>3846.649902</td>\n",
       "      <td>3813.219971</td>\n",
       "      <td>3030300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>-0.561001</td>\n",
       "      <td>-0.384986</td>\n",
       "      <td>3829.560059</td>\n",
       "      <td>3783.219971</td>\n",
       "      <td>3848.320068</td>\n",
       "      <td>3780.780029</td>\n",
       "      <td>3083520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>0.847657</td>\n",
       "      <td>0.183775</td>\n",
       "      <td>3805.449951</td>\n",
       "      <td>3849.280029</td>\n",
       "      <td>3858.189941</td>\n",
       "      <td>3805.449951</td>\n",
       "      <td>3003680000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1761 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  finbert_score  vader_score         Open        Close  \\\n",
       "0     2016-01-04      -0.999072    -0.296000  2038.199951  2012.660034   \n",
       "1     2016-01-05      -0.849274    -0.073340  2013.780029  2016.709961   \n",
       "2     2016-01-06      -0.999978     0.296000  2011.709961  1990.260010   \n",
       "3     2016-01-07      -0.884580    -0.309633  1985.319946  1943.089966   \n",
       "4     2016-01-08      -0.999922    -0.734550  1945.969971  1922.030029   \n",
       "...          ...            ...          ...          ...          ...   \n",
       "1756  2022-12-22      -0.470038     0.037473  3853.260010  3822.389893   \n",
       "1757  2022-12-23       0.495882    -0.023078  3815.110107  3844.820068   \n",
       "1758  2022-12-27      -0.173062     0.155650  3843.340088  3829.250000   \n",
       "1759  2022-12-28      -0.561001    -0.384986  3829.560059  3783.219971   \n",
       "1760  2022-12-29       0.847657     0.183775  3805.449951  3849.280029   \n",
       "\n",
       "             High          Low      Volume  \n",
       "0     2038.199951  1989.680054  4304880000  \n",
       "1     2021.939941  2004.170044  3706620000  \n",
       "2     2011.709961  1979.050049  4336660000  \n",
       "3     1985.319946  1938.829956  5076590000  \n",
       "4     1960.400024  1918.459961  4664940000  \n",
       "...           ...          ...         ...  \n",
       "1756  3853.260010  3764.489990  3956950000  \n",
       "1757  3845.800049  3797.010010  2819280000  \n",
       "1758  3846.649902  3813.219971  3030300000  \n",
       "1759  3848.320068  3780.780029  3083520000  \n",
       "1760  3858.189941  3805.449951  3003680000  \n",
       "\n",
       "[1761 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91228a05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T02:21:33.611601Z",
     "iopub.status.busy": "2023-10-20T02:21:33.611137Z",
     "iopub.status.idle": "2023-10-20T02:21:33.617827Z",
     "shell.execute_reply": "2023-10-20T02:21:33.616982Z"
    },
    "papermill": {
     "duration": 0.014868,
     "end_time": "2023-10-20T02:21:33.619666",
     "exception": false,
     "start_time": "2023-10-20T02:21:33.604798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "# import pandas as pd \n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "# import tensorflow as tf\n",
    "\n",
    "# def create_sequences(data, seq_len):\n",
    "#     sequences = []\n",
    "#     target = []\n",
    "    \n",
    "#     for i in range(len(data) - seq_len):\n",
    "#         sequences.append(data[i:i+seq_len])\n",
    "#         target.append(data[i+seq_len, 0])  # Predict the 'Adj Close' value\n",
    "        \n",
    "#     return np.array(sequences), np.array(target)\n",
    "\n",
    "# def build_and_train_model(X_train, y_train, input_shape, epochs=100, learning_rate=0.02):\n",
    "#     model = tf.keras.models.Sequential([\n",
    "#         tf.keras.layers.LSTM(70, activation=\"tanh\", return_sequences=True, input_shape=input_shape),\n",
    "#         tf.keras.layers.LSTM(30, activation=\"tanh\", return_sequences=True),\n",
    "#         tf.keras.layers.LSTM(10, activation=\"tanh\"),\n",
    "#         tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "#     ])\n",
    "    \n",
    "#     model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "    \n",
    "#     # Implement early stopping\n",
    "#     early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "#     model.fit(\n",
    "#         X_train, y_train,\n",
    "#         epochs=epochs,\n",
    "#         callbacks=[early_stopping],  # add early stopping\n",
    "#         verbose=0\n",
    "#     )\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341c9fcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T02:21:33.631719Z",
     "iopub.status.busy": "2023-10-20T02:21:33.631080Z",
     "iopub.status.idle": "2023-10-20T02:21:33.636204Z",
     "shell.execute_reply": "2023-10-20T02:21:33.635336Z"
    },
    "papermill": {
     "duration": 0.013831,
     "end_time": "2023-10-20T02:21:33.638345",
     "exception": false,
     "start_time": "2023-10-20T02:21:33.624514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def run_scenario(data, label, split=0.85, sequence_length=10, epochs=100, learning_rate=0.02):\n",
    "#     # Your previous data preprocessing and model training code\n",
    "    \n",
    "#     # Split data into training and testing sets\n",
    "#     train_size = int(len(data) * split)\n",
    "#     train_data, test_data = data[:train_size], data[train_size:]\n",
    "\n",
    "#     # Normalize data\n",
    "#     scaler = MinMaxScaler()\n",
    "#     train_scaled = scaler.fit_transform(train_data)\n",
    "#     test_scaled = scaler.transform(test_data)\n",
    "\n",
    "#     # Create sequences\n",
    "#     X_train, y_train = create_sequences(train_scaled, sequence_length)\n",
    "#     X_test, y_test = create_sequences(test_scaled, sequence_length)\n",
    "\n",
    "#     # Build and train model\n",
    "#     model = build_and_train_model(X_train, y_train, (sequence_length, data.shape[1]), epochs, learning_rate)\n",
    "\n",
    "#     # Make predictions\n",
    "#     predictions = model.predict(X_test)\n",
    "#     predictions = scaler.inverse_transform(np.hstack((predictions, np.zeros((len(predictions), data.shape[1]-1)))))[:, 0]\n",
    "\n",
    "#     # Inverse scale y_test\n",
    "#     y_test_original = scaler.inverse_transform(np.hstack((y_test.reshape(-1, 1), np.zeros((len(y_test), data.shape[1]-1)))))[:, 0]\n",
    "\n",
    "#     # Evaluate model\n",
    "#     mae = mean_absolute_error(predictions, y_test_original)\n",
    "#     mape = mean_absolute_percentage_error(predictions, y_test_original)\n",
    "#     acc = 1 - mape\n",
    "\n",
    "#     # Store results\n",
    "#     results[label] = {\"MAE\": mae, \"MAPE\": mape, \"Accuracy\": acc}\n",
    "\n",
    "# # Combine and select relevant data columns\n",
    "# data_no_sentiment = stock_data[['Adj Close']].values\n",
    "# data_finbert = pd.concat([stock_data['Adj Close'], news_df['FinBERT score']], axis=1).values\n",
    "# data_vader = pd.concat([stock_data['Adj Close'], news_df['VADER score']], axis=1).values\n",
    "\n",
    "# # Store results\n",
    "# results = {}\n",
    "\n",
    "# # Run scenarios\n",
    "# run_scenario(data_no_sentiment, \"No Sentiment\")\n",
    "# run_scenario(data_finbert, \"FinBERT Sentiment\")\n",
    "# run_scenario(data_vader, \"VADER Sentiment\")\n",
    "\n",
    "# # Display results\n",
    "# for scenario, metrics in results.items():\n",
    "#     print(f\"\\n{scenario}:\")\n",
    "#     for metric, value in metrics.items():\n",
    "#         print(f\"{metric} = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eabbf19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T02:21:33.650446Z",
     "iopub.status.busy": "2023-10-20T02:21:33.649804Z",
     "iopub.status.idle": "2023-10-20T02:21:33.659229Z",
     "shell.execute_reply": "2023-10-20T02:21:33.658291Z"
    },
    "papermill": {
     "duration": 0.018353,
     "end_time": "2023-10-20T02:21:33.661443",
     "exception": false,
     "start_time": "2023-10-20T02:21:33.643090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "# import math\n",
    "\n",
    "# def set_seeds(seed_value=42):\n",
    "#     \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "#     np.random.seed(seed_value)\n",
    "#     tf.random.set_seed(seed_value)\n",
    "#     random.seed(seed_value)\n",
    "\n",
    "# def create_sequences(data, seq_len):\n",
    "#     \"\"\"Create sequences and targets for LSTM training.\"\"\"\n",
    "#     sequences = []\n",
    "#     target = []\n",
    "#     for i in range(len(data) - seq_len):\n",
    "#         sequences.append(data[i:i+seq_len])\n",
    "#         target.append(data[i+seq_len, 0])\n",
    "#     return np.array(sequences), np.array(target)\n",
    "\n",
    "# def build_and_train_model(X_train, y_train, input_shape, epochs, learning_rate):\n",
    "#     \"\"\"\n",
    "#     Build and train the LSTM model.\n",
    "\n",
    "#     Parameters:\n",
    "#     - X_train (np.array): Training data.\n",
    "#     - y_train (np.array): Training labels.\n",
    "#     - input_shape (tuple): Shape of input data.\n",
    "#     - epochs (int): Number of epochs.\n",
    "#     - learning_rate (float): Learning rate.\n",
    "#     - use_bayesian_optimization (bool): Whether to use Bayesian Optimization.\n",
    "#     - bayes_optimizer (BayesianOptimization object): Bayesian Optimization object for hyperparameter tuning.\n",
    "\n",
    "#     Returns:\n",
    "#     - model (tf.keras.Model): Trained LSTM model.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     model = tf.keras.models.Sequential(\n",
    "#         [\n",
    "#             tf.keras.layers.LSTM(units = 140, activation = \"tanh\", return_sequences = True, input_shape=input_shape),\n",
    "#             tf.keras.layers.Dropout(0.15),\n",
    "#             tf.keras.layers.LSTM(units = 60, activation = \"tanh\", return_sequences = True),\n",
    "#             tf.keras.layers.Dropout(0.05),\n",
    "#             tf.keras.layers.LSTM(units = 20, activation = \"tanh\", return_sequences = False),\n",
    "#             tf.keras.layers.Dropout(0.01),\n",
    "#             tf.keras.layers.Dense(units = 1, activation = \"linear\")\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "    \n",
    "#     early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "#     model.fit(X_train, y_train, epochs=epochs, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# def optimize_hyperparameters(X_train, y_train, input_shape, X_val, y_val, scaler):\n",
    "#     def train_model_and_get_accuracy(learning_rate, epochs):\n",
    "#         learning_rate = 10**(-learning_rate)\n",
    "#         epochs = int(epochs)\n",
    "#         model = build_and_train_model(X_train, y_train, input_shape, epochs, learning_rate)\n",
    "#         predictions = model.predict(X_val)\n",
    "#         predictions = scaler.inverse_transform(predictions)[:, 0]\n",
    "#         y_val_original = scaler.inverse_transform(y_val.reshape(-1, 1))[:, 0]\n",
    "#         mape = mean_absolute_percentage_error(y_val_original, predictions)\n",
    "#         acc = 1 - mape\n",
    "#         return acc\n",
    "    \n",
    "#     optimizer = BayesianOptimization(\n",
    "#         f=train_model_and_get_accuracy, \n",
    "#         pbounds={\"learning_rate\": (1, 5), \"epochs\": (10, 150)}, \n",
    "#         random_state=42, \n",
    "#         verbose=2\n",
    "#     )\n",
    "#     optimizer.maximize(init_points=5, n_iter=15)\n",
    "#     best_params = optimizer.max['params']\n",
    "#     best_params['learning_rate'] = 10**(-best_params['learning_rate'])\n",
    "#     best_params['epochs'] = int(best_params['epochs'])\n",
    "#     return best_params\n",
    "\n",
    "# def get_timeframe(data):\n",
    "#     \"\"\"\n",
    "#     Get the minimum and maximum dates from the data.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data (np.array): Time series data with date in the first column.\n",
    "\n",
    "#     Returns:\n",
    "#     - (str, str): Minimum and maximum date as strings.\n",
    "#     \"\"\"\n",
    "#     min_date = pd.to_datetime(np.min(data[:, 0])).strftime('%Y-%m-%d')\n",
    "#     max_date = pd.to_datetime(np.max(data[:, 0])).strftime('%Y-%m-%d')\n",
    "#     return min_date, max_date\n",
    "\n",
    "# def run_scenario(data, label, split=0.85, sequence_length=10, use_bayesian_optimization=True):\n",
    "#     results = {}\n",
    "#     train_size = int(len(data) * split)\n",
    "#     train_data, test_data = data[:train_size], data[train_size:]\n",
    "    \n",
    "#     # Get and print timeframes\n",
    "#     train_min_date, train_max_date = get_timeframe(train_data)\n",
    "#     test_min_date, test_max_date = get_timeframe(test_data)\n",
    "#     print(f\"Training data covers from {train_min_date} to {train_max_date}\")\n",
    "#     print(f\"Test data covers from {test_min_date} to {test_max_date}\")\n",
    "\n",
    "#     # Separate date and numeric data\n",
    "#     train_dates, train_data_numeric = train_data[:, 0], train_data[:, 1:].astype(float)\n",
    "#     test_dates, test_data_numeric = test_data[:, 0], test_data[:, 1:].astype(float)\n",
    "    \n",
    "#     # Scale data\n",
    "#     scaler = MinMaxScaler()\n",
    "#     train_scaled_numeric = scaler.fit_transform(train_data_numeric)\n",
    "#     test_scaled_numeric = scaler.transform(test_data_numeric)\n",
    "    \n",
    "#     # Additional scaler for 'Close' prices only\n",
    "#     close_scaler = MinMaxScaler()\n",
    "#     close_scaler.fit(train_data_numeric[:, 1].reshape(-1, 1))  # Assuming 'Close' is the second column in train_data_numeric\n",
    "    \n",
    "#     # Splitting training data into training and validation sets\n",
    "#     val_size = int(len(train_scaled_numeric) * 0.2)\n",
    "#     val_scaled_numeric, train_scaled_numeric = train_scaled_numeric[-val_size:], train_scaled_numeric[:-val_size]\n",
    "#     val_dates, train_dates = train_dates[-val_size:], train_dates[:-val_size]\n",
    "    \n",
    "#     # Get and print timeframe for validation data\n",
    "#     val_min_date, val_max_date = get_timeframe(train_data[-val_size:])\n",
    "#     print(f\"Validation data covers from {val_min_date} to {val_max_date}\")\n",
    "    \n",
    "#     # Creating sequences\n",
    "#     X_train, y_train = create_sequences(train_scaled_numeric, sequence_length)\n",
    "#     X_val, y_val = create_sequences(val_scaled_numeric, sequence_length)\n",
    "#     X_test, y_test = create_sequences(test_scaled_numeric, sequence_length)\n",
    "    \n",
    "#     # Hyperparameter optimization and model training\n",
    "#     if use_bayesian_optimization:\n",
    "#         best_params = optimize_hyperparameters(X_train, y_train, (sequence_length, train_data_numeric.shape[1]), X_val, y_val, close_scaler)\n",
    "#         model = build_and_train_model(X_train, y_train, (sequence_length, train_data_numeric.shape[1]), best_params['epochs'], best_params['learning_rate'])\n",
    "#     else:\n",
    "#         # Using default or predetermined hyperparameters for training\n",
    "#         model = build_and_train_model(X_train, y_train, (sequence_length, train_data_numeric.shape[1]), 50, 0.001)  # Example values for epochs and learning_rate\n",
    "    \n",
    "#     # Model evaluation\n",
    "#     model.save(os.path.join(\"saved_models\", f\"{label}.h5\"))\n",
    "    \n",
    "#     predictions = model.predict(X_test)\n",
    "#     predictions_original = close_scaler.inverse_transform(predictions)\n",
    "#     y_test_original = close_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "#     mae = mean_absolute_error(y_test_original, predictions_original)\n",
    "#     mape = mean_absolute_percentage_error(y_test_original, predictions_original)\n",
    "#     acc = 1 - mape\n",
    "#     rmse = math.sqrt(mean_squared_error(y_test_original, predictions_original))\n",
    "#     r2 = r2_score(y_test_original, predictions_original)\n",
    "    \n",
    "#     results[label] = {\n",
    "#         \"MAE\": mae, \n",
    "#         \"MAPE\": mape, \n",
    "#         \"Accuracy\": acc,\n",
    "#         \"RMSE\": rmse,  # Added line\n",
    "#         \"R2 Score\": r2,  # Added line\n",
    "#         \"Best Parameters\": best_params if use_bayesian_optimization else None\n",
    "#     }\n",
    "    \n",
    "#     # Creating a DataFrame for predictions\n",
    "#     predictions_df = pd.DataFrame({\n",
    "#         'Date': test_dates[sequence_length:],\n",
    "#         'Close': y_test_original.flatten(),\n",
    "#         f'{label}_predicted': predictions_original.flatten()\n",
    "#     })\n",
    "    \n",
    "#     return results, predictions_df\n",
    "\n",
    "\n",
    "# os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "# set_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "765aa564",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T02:21:33.673577Z",
     "iopub.status.busy": "2023-10-20T02:21:33.672937Z",
     "iopub.status.idle": "2023-10-20T02:21:33.677116Z",
     "shell.execute_reply": "2023-10-20T02:21:33.676287Z"
    },
    "papermill": {
     "duration": 0.012824,
     "end_time": "2023-10-20T02:21:33.679105",
     "exception": false,
     "start_time": "2023-10-20T02:21:33.666281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Ensure date is in datetime format\n",
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# # Scenario 1: No Sentiment\n",
    "# data_no_sentiment = df[['date', 'Open', 'Close', 'High', 'Low', 'Volume']].values\n",
    "# results_no_sentiment, predictions_no_sentiment = run_scenario(data_no_sentiment, \"No_Sentiment\", use_bayesian_optimization=True)\n",
    "\n",
    "# # Scenario 2: With FinBERT Score\n",
    "# data_finbert = df[['date', 'Open', 'Close', 'High', 'Low', 'Volume', 'finbert_score']].values\n",
    "# results_finbert, predictions_finbert = run_scenario(data_finbert, \"FinBERT_Sentiment\", use_bayesian_optimization=True)\n",
    "\n",
    "# # Scenario 3: With VADER Score\n",
    "# data_vader = df[['date', 'Open', 'Close', 'High', 'Low', 'Volume', 'vader_score']].values\n",
    "# results_vader, predictions_vader = run_scenario(data_vader, \"VADER_Sentiment\", use_bayesian_optimization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "145969c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T02:21:33.691320Z",
     "iopub.status.busy": "2023-10-20T02:21:33.690697Z",
     "iopub.status.idle": "2023-10-20T02:21:45.134679Z",
     "shell.execute_reply": "2023-10-20T02:21:45.133514Z"
    },
    "papermill": {
     "duration": 11.453462,
     "end_time": "2023-10-20T02:21:45.137305",
     "exception": false,
     "start_time": "2023-10-20T02:21:33.683843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "import math\n",
    "\n",
    "def set_seeds(seed_value=42):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "\n",
    "def create_sequences(data, seq_len):\n",
    "    \"\"\"Create sequences and targets for LSTM training.\"\"\"\n",
    "    sequences = []\n",
    "    target = []\n",
    "    for i in range(len(data) - seq_len):\n",
    "        sequences.append(data[i:i+seq_len])\n",
    "        target.append(data[i+seq_len, 0])\n",
    "    return np.array(sequences), np.array(target)\n",
    "\n",
    "def build_and_train_model(X_train, y_train, input_shape, epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Build and train the LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (np.array): Training data.\n",
    "    - y_train (np.array): Training labels.\n",
    "    - input_shape (tuple): Shape of input data.\n",
    "    - epochs (int): Number of epochs.\n",
    "    - learning_rate (float): Learning rate.\n",
    "    - use_bayesian_optimization (bool): Whether to use Bayesian Optimization.\n",
    "    - bayes_optimizer (BayesianOptimization object): Bayesian Optimization object for hyperparameter tuning.\n",
    "\n",
    "    Returns:\n",
    "    - model (tf.keras.Model): Trained LSTM model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.LSTM(units = 140, activation = \"tanh\", return_sequences = True, input_shape=input_shape),\n",
    "            tf.keras.layers.Dropout(0.15),\n",
    "            tf.keras.layers.LSTM(units = 60, activation = \"tanh\", return_sequences = True),\n",
    "            tf.keras.layers.Dropout(0.05),\n",
    "            tf.keras.layers.LSTM(units = 20, activation = \"tanh\", return_sequences = False),\n",
    "            tf.keras.layers.Dropout(0.01),\n",
    "            tf.keras.layers.Dense(units = 1, activation = \"linear\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=epochs, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def optimize_hyperparameters(X_train, y_train, input_shape, X_val, y_val, scaler):\n",
    "    def train_model_and_get_accuracy(learning_rate, epochs):\n",
    "        learning_rate = 10**(-learning_rate)\n",
    "        epochs = int(epochs)\n",
    "        model = build_and_train_model(X_train, y_train, input_shape, epochs, learning_rate)\n",
    "        predictions = model.predict(X_val)\n",
    "        predictions = scaler.inverse_transform(predictions)[:, 0]\n",
    "        y_val_original = scaler.inverse_transform(y_val.reshape(-1, 1))[:, 0]\n",
    "        mape = mean_absolute_percentage_error(y_val_original, predictions)\n",
    "        acc = 1 - mape\n",
    "        return acc\n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "        f=train_model_and_get_accuracy, \n",
    "        pbounds={\"learning_rate\": (1, 5), \"epochs\": (10, 150)}, \n",
    "        random_state=42, \n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    best_params = optimizer.max['params']\n",
    "    best_params['learning_rate'] = 10**(-best_params['learning_rate'])\n",
    "    best_params['epochs'] = int(best_params['epochs'])\n",
    "    return best_params\n",
    "\n",
    "def get_timeframe(data):\n",
    "    \"\"\"\n",
    "    Get the minimum and maximum dates from the data.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.array): Time series data with date in the first column.\n",
    "\n",
    "    Returns:\n",
    "    - (str, str): Minimum and maximum date as strings.\n",
    "    \"\"\"\n",
    "    min_date = pd.to_datetime(np.min(data[:, 0])).strftime('%Y-%m-%d')\n",
    "    max_date = pd.to_datetime(np.max(data[:, 0])).strftime('%Y-%m-%d')\n",
    "    return min_date, max_date\n",
    "\n",
    "def run_scenario(data, label, split=0.85, sequence_length=10, use_bayesian_optimization=True):\n",
    "    results = {}\n",
    "    train_size = int(len(data) * split)\n",
    "    train_data, test_data = data[:train_size], data[train_size:]\n",
    "    \n",
    "    # Get and print timeframes\n",
    "    train_min_date, train_max_date = get_timeframe(train_data)\n",
    "    test_min_date, test_max_date = get_timeframe(test_data)\n",
    "    print(f\"Training data covers from {train_min_date} to {train_max_date}\")\n",
    "    print(f\"Test data covers from {test_min_date} to {test_max_date}\")\n",
    "\n",
    "    # Separate date and numeric data\n",
    "    train_dates, train_data_numeric = train_data[:, 0], train_data[:, 1:].astype(float)\n",
    "    test_dates, test_data_numeric = test_data[:, 0], test_data[:, 1:].astype(float)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled_numeric = scaler.fit_transform(train_data_numeric)\n",
    "    test_scaled_numeric = scaler.transform(test_data_numeric)\n",
    "    \n",
    "    # Additional scaler for 'Close' prices only\n",
    "    close_scaler = MinMaxScaler()\n",
    "    close_scaler.fit(train_data_numeric[:, 0].reshape(-1, 1))  # Assuming 'Close' is the first column in train_data_numeric\n",
    "    \n",
    "    # Splitting training data into training and validation sets\n",
    "    val_size = int(len(train_scaled_numeric) * 0.2)\n",
    "    val_scaled_numeric, train_scaled_numeric = train_scaled_numeric[-val_size:], train_scaled_numeric[:-val_size]\n",
    "    val_dates, train_dates = train_dates[-val_size:], train_dates[:-val_size]\n",
    "    \n",
    "    # Get and print timeframe for validation data\n",
    "    val_min_date, val_max_date = get_timeframe(train_data[-val_size:])\n",
    "    print(f\"Validation data covers from {val_min_date} to {val_max_date}\")\n",
    "    \n",
    "    # Creating sequences\n",
    "    X_train, y_train = create_sequences(train_scaled_numeric, sequence_length)\n",
    "    X_val, y_val = create_sequences(val_scaled_numeric, sequence_length)\n",
    "    X_test, y_test = create_sequences(test_scaled_numeric, sequence_length)\n",
    "    \n",
    "    # Hyperparameter optimization and model training\n",
    "    if use_bayesian_optimization:\n",
    "        best_params = optimize_hyperparameters(X_train, y_train, (sequence_length, train_data_numeric.shape[1]), X_val, y_val, close_scaler)\n",
    "        model = build_and_train_model(X_train, y_train, (sequence_length, train_data_numeric.shape[1]), best_params['epochs'], best_params['learning_rate'])\n",
    "    else:\n",
    "        # Using default or predetermined hyperparameters for training\n",
    "        model = build_and_train_model(X_train, y_train, (sequence_length, train_data_numeric.shape[1]), 50, 0.001)  # Example values for epochs and learning_rate\n",
    "    \n",
    "    # Model evaluation\n",
    "    model.save(os.path.join(\"saved_models\", f\"{label}.h5\"))\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    predictions_original = close_scaler.inverse_transform(predictions)\n",
    "    y_test_original = close_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    mae = mean_absolute_error(y_test_original, predictions_original)\n",
    "    mape = mean_absolute_percentage_error(y_test_original, predictions_original)\n",
    "    acc = 1 - mape\n",
    "    rmse = math.sqrt(mean_squared_error(y_test_original, predictions_original))\n",
    "    r2 = r2_score(y_test_original, predictions_original)\n",
    "    \n",
    "    results[label] = {\n",
    "        \"MAE\": mae, \n",
    "        \"MAPE\": mape, \n",
    "        \"Accuracy\": acc,\n",
    "        \"RMSE\": rmse,  # Added line\n",
    "        \"R2 Score\": r2,  # Added line\n",
    "        \"Best Parameters\": best_params if use_bayesian_optimization else None\n",
    "    }\n",
    "    \n",
    "    # Creating a DataFrame for predictions\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Date': test_dates[sequence_length:],\n",
    "        'Close': y_test_original.flatten(),\n",
    "        f'{label}_predicted': predictions_original.flatten()\n",
    "    })\n",
    "    \n",
    "    return results, predictions_df\n",
    "\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "471871c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T02:21:45.149570Z",
     "iopub.status.busy": "2023-10-20T02:21:45.148754Z",
     "iopub.status.idle": "2023-10-20T02:57:05.258348Z",
     "shell.execute_reply": "2023-10-20T02:57:05.257043Z"
    },
    "papermill": {
     "duration": 2120.118513,
     "end_time": "2023-10-20T02:57:05.260865",
     "exception": false,
     "start_time": "2023-10-20T02:21:45.142352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data covers from 2016-01-04 to 2021-12-09\n",
      "Test data covers from 2021-12-10 to 2022-12-29\n",
      "Validation data covers from 2020-10-05 to 2021-12-09\n",
      "|   iter    |  target   |  epochs   | learni... |\n",
      "-------------------------------------------------\n",
      "10/10 [==============================] - 1s 9ms/step\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.9763   \u001b[0m | \u001b[0m62.44    \u001b[0m | \u001b[0m4.803    \u001b[0m |\n",
      "10/10 [==============================] - 1s 9ms/step\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.9665   \u001b[0m | \u001b[0m112.5    \u001b[0m | \u001b[0m3.395    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.8481   \u001b[0m | \u001b[0m31.84    \u001b[0m | \u001b[0m1.624    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m0.9864   \u001b[0m | \u001b[95m18.13    \u001b[0m | \u001b[95m4.465    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.9714   \u001b[0m | \u001b[0m94.16    \u001b[0m | \u001b[0m3.832    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.9882   \u001b[0m | \u001b[95m62.46    \u001b[0m | \u001b[95m4.702    \u001b[0m |\n",
      "10/10 [==============================] - 1s 9ms/step\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.9708   \u001b[0m | \u001b[0m62.42    \u001b[0m | \u001b[0m3.99     \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.9858   \u001b[0m | \u001b[0m63.08    \u001b[0m | \u001b[0m4.532    \u001b[0m |\n",
      "10/10 [==============================] - 2s 10ms/step\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.9781   \u001b[0m | \u001b[0m19.08    \u001b[0m | \u001b[0m4.568    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.9554   \u001b[0m | \u001b[0m18.52    \u001b[0m | \u001b[0m3.599    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.811    \u001b[0m | \u001b[0m17.31    \u001b[0m | \u001b[0m4.798    \u001b[0m |\n",
      "10/10 [==============================] - 1s 9ms/step\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.9648   \u001b[0m | \u001b[0m63.72    \u001b[0m | \u001b[0m3.775    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.9776   \u001b[0m | \u001b[0m63.98    \u001b[0m | \u001b[0m4.946    \u001b[0m |\n",
      "10/10 [==============================] - 1s 9ms/step\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.984    \u001b[0m | \u001b[0m19.1     \u001b[0m | \u001b[0m4.572    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.9878   \u001b[0m | \u001b[0m19.62    \u001b[0m | \u001b[0m4.569    \u001b[0m |\n",
      "10/10 [==============================] - 1s 9ms/step\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.6514   \u001b[0m | \u001b[0m94.57    \u001b[0m | \u001b[0m1.607    \u001b[0m |\n",
      "10/10 [==============================] - 2s 10ms/step\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.9871   \u001b[0m | \u001b[0m64.86    \u001b[0m | \u001b[0m4.32     \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.9817   \u001b[0m | \u001b[0m65.56    \u001b[0m | \u001b[0m5.0      \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.9671   \u001b[0m | \u001b[0m65.72    \u001b[0m | \u001b[0m3.883    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.9879   \u001b[0m | \u001b[0m18.12    \u001b[0m | \u001b[0m4.49     \u001b[0m |\n",
      "=================================================\n",
      "8/8 [==============================] - 1s 11ms/step\n",
      "Training data covers from 2016-01-04 to 2021-12-09\n",
      "Test data covers from 2021-12-10 to 2022-12-29\n",
      "Validation data covers from 2020-10-05 to 2021-12-09\n",
      "|   iter    |  target   |  epochs   | learni... |\n",
      "-------------------------------------------------\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.9779   \u001b[0m | \u001b[0m62.44    \u001b[0m | \u001b[0m4.803    \u001b[0m |\n",
      "10/10 [==============================] - 1s 9ms/step\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.9764   \u001b[0m | \u001b[0m112.5    \u001b[0m | \u001b[0m3.395    \u001b[0m |\n",
      "10/10 [==============================] - 1s 9ms/step\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.9726   \u001b[0m | \u001b[0m31.84    \u001b[0m | \u001b[0m1.624    \u001b[0m |\n",
      "10/10 [==============================] - 1s 9ms/step\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.9713   \u001b[0m | \u001b[0m18.13    \u001b[0m | \u001b[0m4.465    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.9861   \u001b[0m | \u001b[95m94.16    \u001b[0m | \u001b[95m3.832    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.682    \u001b[0m | \u001b[0m83.38    \u001b[0m | \u001b[0m1.131    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.9774   \u001b[0m | \u001b[0m94.07    \u001b[0m | \u001b[0m3.926    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.6551   \u001b[0m | \u001b[0m96.34    \u001b[0m | \u001b[0m1.584    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.9722   \u001b[0m | \u001b[0m64.1     \u001b[0m | \u001b[0m3.479    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.9696   \u001b[0m | \u001b[0m61.87    \u001b[0m | \u001b[0m2.526    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.9857   \u001b[0m | \u001b[0m59.98    \u001b[0m | \u001b[0m4.313    \u001b[0m |\n",
      "10/10 [==============================] - 2s 10ms/step\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.9831   \u001b[0m | \u001b[0m59.22    \u001b[0m | \u001b[0m2.014    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.984    \u001b[0m | \u001b[0m57.49    \u001b[0m | \u001b[0m3.851    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.6555   \u001b[0m | \u001b[0m56.52    \u001b[0m | \u001b[0m1.335    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.9704   \u001b[0m | \u001b[0m114.8    \u001b[0m | \u001b[0m3.554    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.7982   \u001b[0m | \u001b[0m113.7    \u001b[0m | \u001b[0m1.311    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.9772   \u001b[0m | \u001b[0m113.4    \u001b[0m | \u001b[0m4.942    \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[95m18       \u001b[0m | \u001b[95m0.987    \u001b[0m | \u001b[95m111.0    \u001b[0m | \u001b[95m4.75     \u001b[0m |\n",
      "10/10 [==============================] - 1s 10ms/step\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.9745   \u001b[0m | \u001b[0m109.6    \u001b[0m | \u001b[0m2.88     \u001b[0m |\n",
      "10/10 [==============================] - 2s 10ms/step\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.9717   \u001b[0m | \u001b[0m108.4    \u001b[0m | \u001b[0m5.0      \u001b[0m |\n",
      "=================================================\n",
      "8/8 [==============================] - 1s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "# Ensure date is in datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# # Scenario 1: Just Close\n",
    "# data_close_only = df[['date', 'Close']].values\n",
    "# results_close_only, predictions_close_only = run_scenario(data_close_only, \"Close_Only\", use_bayesian_optimization=True)\n",
    "\n",
    "# Scenario 2: Close + FinBERT Score\n",
    "data_finbert = df[['date', 'Close', 'finbert_score']].values\n",
    "results_finbert, predictions_finbert = run_scenario(data_finbert, \"FinBERT_Sentiment\", use_bayesian_optimization=True)\n",
    "\n",
    "# Scenario 3: Close + VADER Score\n",
    "data_vader = df[['date', 'Close', 'vader_score']].values\n",
    "results_vader, predictions_vader = run_scenario(data_vader, \"VADER_Sentiment\", use_bayesian_optimization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9db4857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T02:57:05.290138Z",
     "iopub.status.busy": "2023-10-20T02:57:05.289715Z",
     "iopub.status.idle": "2023-10-20T02:57:05.308220Z",
     "shell.execute_reply": "2023-10-20T02:57:05.306746Z"
    },
    "papermill": {
     "duration": 0.036625,
     "end_time": "2023-10-20T02:57:05.311054",
     "exception": false,
     "start_time": "2023-10-20T02:57:05.274429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine and Display Results\n",
    "results = {\n",
    "#     **results_no_sentiment, \n",
    "    **results_finbert, \n",
    "    **results_vader}\n",
    "# predictions = predictions_no_sentiment.merge(predictions_finbert[['Date', 'FinBERT_Sentiment_predicted']], on='Date')\n",
    "# predictions = predictions.merge(predictions_vader[['Date', 'VADER_Sentiment_predicted']], on='Date')\n",
    "\n",
    "predictions = predictions_finbert.merge(predictions_vader[['Date', 'VADER_Sentiment_predicted']], on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0da7a05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T02:57:05.340756Z",
     "iopub.status.busy": "2023-10-20T02:57:05.340294Z",
     "iopub.status.idle": "2023-10-20T02:57:05.354063Z",
     "shell.execute_reply": "2023-10-20T02:57:05.352920Z"
    },
    "papermill": {
     "duration": 0.031815,
     "end_time": "2023-10-20T02:57:05.356736",
     "exception": false,
     "start_time": "2023-10-20T02:57:05.324921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the results dictionary to a DataFrame\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "\n",
    "# Save the results DataFrame to a CSV file\n",
    "results_df.to_csv('results.csv')\n",
    "predictions.to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34d3f859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T02:57:05.386413Z",
     "iopub.status.busy": "2023-10-20T02:57:05.386041Z",
     "iopub.status.idle": "2023-10-20T02:57:05.399404Z",
     "shell.execute_reply": "2023-10-20T02:57:05.398025Z"
    },
    "papermill": {
     "duration": 0.030884,
     "end_time": "2023-10-20T02:57:05.401974",
     "exception": false,
     "start_time": "2023-10-20T02:57:05.371090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>Best Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FinBERT_Sentiment</th>\n",
       "      <td>125.392360</td>\n",
       "      <td>0.030597</td>\n",
       "      <td>0.969403</td>\n",
       "      <td>159.071214</td>\n",
       "      <td>0.725601</td>\n",
       "      <td>{'epochs': 62, 'learning_rate': 1.985558620320...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VADER_Sentiment</th>\n",
       "      <td>112.894339</td>\n",
       "      <td>0.027714</td>\n",
       "      <td>0.972286</td>\n",
       "      <td>136.966174</td>\n",
       "      <td>0.796565</td>\n",
       "      <td>{'epochs': 110, 'learning_rate': 1.77767041710...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          MAE      MAPE  Accuracy        RMSE  R2 Score  \\\n",
       "FinBERT_Sentiment  125.392360  0.030597  0.969403  159.071214  0.725601   \n",
       "VADER_Sentiment    112.894339  0.027714  0.972286  136.966174  0.796565   \n",
       "\n",
       "                                                     Best Parameters  \n",
       "FinBERT_Sentiment  {'epochs': 62, 'learning_rate': 1.985558620320...  \n",
       "VADER_Sentiment    {'epochs': 110, 'learning_rate': 1.77767041710...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa5b08",
   "metadata": {
    "papermill": {
     "duration": 0.013348,
     "end_time": "2023-10-20T02:57:05.429346",
     "exception": false,
     "start_time": "2023-10-20T02:57:05.415998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\tMAE\tMAPE\tAccuracy\tBest Parameters\n",
    "No_Sentiment\t469.795235\t0.108335\t0.891665\t{'epochs': 62, 'learning_rate': 1.574500396576...\n",
    "FinBERT_Sentiment\t587.465718\t0.135496\t0.864504\t{'epochs': 94, 'learning_rate': 0.000147132864...\n",
    "VADER_Sentiment\t249.323561\t0.057158\t0.942842\t{'epochs': 62, 'learning_rate': 1.056115575977.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56ff2b86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T02:57:05.459090Z",
     "iopub.status.busy": "2023-10-20T02:57:05.458665Z",
     "iopub.status.idle": "2023-10-20T02:57:05.467788Z",
     "shell.execute_reply": "2023-10-20T02:57:05.466439Z"
    },
    "papermill": {
     "duration": 0.027026,
     "end_time": "2023-10-20T02:57:05.470195",
     "exception": false,
     "start_time": "2023-10-20T02:57:05.443169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "# import pandas as pd \n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.metrics import mean_absolute_percentage_error\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # hyperparameters\n",
    "# split = (0.85);\n",
    "# sequence_length = 10;\n",
    "# epochs = 100\n",
    "# learning_rate = 0.02\n",
    "\n",
    "\n",
    "\n",
    "# # loading stock price and news data\n",
    "# stock_data = pd.read_csv(\"/kaggle/input/financialnews/upload_DJIA_table.csv\")\n",
    "# news_data = pd.read_csv(\"/kaggle/working/sentiment.csv\")\n",
    "# stock_column = ['Adj Close']\n",
    "# news_column = ['VADER score']\n",
    "\n",
    "\n",
    "# len_stock_data = stock_data.shape[0]\n",
    "\n",
    "\n",
    "# # splitting data to train and test\n",
    "# train_examples = int(len_stock_data * split)\n",
    "# train = stock_data.get(stock_column).values[:train_examples]\n",
    "# train_sentiment = news_data.get(news_column).values[:train_examples]\n",
    "# test = stock_data.get(stock_column).values[train_examples:]\n",
    "# test_sentiment = news_data.get(news_column).values[train_examples:]\n",
    "# len_train = train.shape[0]\n",
    "# len_test = test.shape[0]\n",
    "# len_train_sentiment = train_sentiment.shape[0]\n",
    "# len_test_sentiment = test_sentiment.shape[0]\n",
    "\n",
    "\n",
    "# # normalizing data\n",
    "# scaler = MinMaxScaler()\n",
    "# train, test = scaler.fit_transform(train), scaler.fit_transform(test)\n",
    "\n",
    "\n",
    "# # splitting training data to x and y\n",
    "# X_train = []\n",
    "# for i in range(len_train - sequence_length):\n",
    "#     X_train.append(train[i : i + sequence_length])\n",
    "# len_X_train = len(X_train)\n",
    "# y_train = np.array(train[sequence_length:]).astype(float)\n",
    "\n",
    "\n",
    "# # splitting testing data to x and y\n",
    "# X_test = []\n",
    "# for i in range(len_test - sequence_length):\n",
    "#     X_test.append(test[i : i + sequence_length])\n",
    "# len_X_test = len(X_test)\n",
    "# y_test = np.array(test[sequence_length:]).astype(float)\n",
    "\n",
    "\n",
    "# # adding news sentiment to train and test\n",
    "# for i in range(len_X_train):\n",
    "#     X_train[i] = X_train[i].tolist()\n",
    "#     X_train[i].append(train_sentiment[sequence_length + i].tolist())\n",
    "# X_train = np.array(X_train).astype(float)\n",
    "\n",
    "# for i in range(len_X_test):\n",
    "#     X_test[i] = X_test[i].tolist()\n",
    "#     X_test[i].append(test_sentiment[sequence_length + i].tolist())\n",
    "# X_test = np.array(X_test).astype(float)\n",
    "\n",
    "\n",
    "# #creating LSTM model\n",
    "# def model_create():\n",
    "#     tf.random.set_seed(1234)\n",
    "#     model = tf.keras.models.Sequential(\n",
    "#         [\n",
    "#             tf.keras.Input(shape = (X_train.shape[1], 1)),\n",
    "#             tf.keras.layers.LSTM(units = 70, activation = \"tanh\", return_sequences = True),\n",
    "#             tf.keras.layers.LSTM(units = 30, activation = \"tanh\", return_sequences = True),\n",
    "#             tf.keras.layers.LSTM(units = 10, activation = \"tanh\", return_sequences = False),\n",
    "#             tf.keras.layers.Dense(units = 1, activation = \"linear\")\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     model.compile(\n",
    "#         loss = tf.keras.losses.mean_squared_error,\n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "#     )\n",
    "\n",
    "#     model.fit(\n",
    "#         X_train, y_train,\n",
    "#         epochs = epochs\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # inverting normaliztion\n",
    "# y_test = scaler.inverse_transform(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# # prediction on test set\n",
    "# def predict(model):\n",
    "#     predictions = model.predict(X_test)\n",
    "#     predictions = scaler.inverse_transform(predictions.reshape(-1,1)).reshape(-1,1)\n",
    "#     return predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # evaluation\n",
    "# def evaluate(predictions):\n",
    "#     mae = mean_absolute_error(predictions, y_test)\n",
    "#     mape = mean_absolute_percentage_error(predictions, y_test)\n",
    "#     return mae, mape, (1 - mape)\n",
    "\n",
    "\n",
    "# # trial runs\n",
    "# def run_model(n):\n",
    "#     total_mae = total_mape = total_acc = 0\n",
    "#     for i in range(n):\n",
    "#         model = model_create()\n",
    "#         predictions = predict(model)\n",
    "#         mae, mape, acc = evaluate(predictions)\n",
    "#         total_mae += mae\n",
    "#         total_mape += mape \n",
    "#         total_acc += acc \n",
    "#     return (total_mae / n), (total_mape / n), (total_acc / n)\n",
    "\n",
    "\n",
    "# mae, mape, acc = run_model(1)\n",
    "\n",
    "# print(f\"Mean Absolute Error = {mae}\")\n",
    "# print(f\"Mean Absolute Percentage Error = {mape}%\")\n",
    "# print(f\"Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa3344c",
   "metadata": {
    "papermill": {
     "duration": 0.013576,
     "end_time": "2023-10-20T02:57:05.499550",
     "exception": false,
     "start_time": "2023-10-20T02:57:05.485974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "FinBert\n",
    "- Mean Absolute Error = 164.18811497631927\n",
    "- Mean Absolute Percentage Error = 0.018404834389613788%\n",
    "- Accuracy = 0.9815951656103862\n",
    "\n",
    "VADER\n",
    "- Mean Absolute Error = 152.3828716275952\n",
    "- Mean Absolute Percentage Error = 0.017230813411148894%\n",
    "- Accuracy = 0.9827691865888512\n",
    "\n",
    "LSTM\n",
    "- Mean Absolute Error = 158.84918020772062\n",
    "- Mean Absolute Percentage Error = 0.01793745272032302%\n",
    "- Accuracy = 0.982062547279677"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2139.506093,
   "end_time": "2023-10-20T02:57:09.201688",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-20T02:21:29.695595",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
